{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from datasets import load_dataset,DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, DataCollatorWithPadding, AutoModelForSequenceClassification, AdamW, get_scheduler, BertForSequenceClassification, pipeline\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "#import evaluate\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "#Might not be able to import evaluate (might not work on mimi)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve data\n",
    "\n",
    "train_dataset = load_dataset('ag_news', split='train')\n",
    "test_dataset = load_dataset('ag_news', split='test')\n",
    "\n",
    "split = train_dataset.train_test_split(test_size=0.1, seed=SEED)\n",
    "\n",
    "train_dataset = split['train']\n",
    "validation_dataset = split['test']\n",
    "\n",
    "#ONLY USE THE FIRST 10000K EXAMPLES OR ELSE TOO LONG (20k was too slow)\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(20000))  # Use first 10k examples\n",
    "validation_dataset = validation_dataset.shuffle(seed=42).select(range(4000))  # Use first 5k examples as validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get shape\n",
    "\n",
    "print(train_dataset.shape)\n",
    "print(test_dataset.shape)\n",
    "print(validation_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets for fine-tuning\n",
    "datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learn about the classes\n",
    "\n",
    "num_labels = datasets['train'].features['label'].num_classes\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "for label_id,label in enumerate(datasets['train'].features['label'].names):\n",
    "    id2label[label_id] = label\n",
    "    label2id[label] = label_id\n",
    "    \n",
    "print(f\"NUM_LABELS: {num_labels}\")\n",
    "print(f\"ID2LABEL: {id2label}\")\n",
    "print(f\"LABEL2ID: {label2id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print some example text blurbs\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "# get random integers in the range of 0 to train_dataset_length\n",
    "EXAMPLE_INDICES = [random.randrange(len(datasets['train'])) for _ in range(3)]\n",
    "\n",
    "for i in EXAMPLE_INDICES:\n",
    "    text = datasets['train']['text'][i]\n",
    "    label_id = datasets['train']['label'][i]\n",
    "    label = id2label[label_id]\n",
    "    print(f\" TEXT[{i}]: {text}\")\n",
    "    print(f\"LABEL[{i}]: {label} ({label_id})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the pre-trained model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "#model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#switch to mimi gpu if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "#Here we switch to the mac GPU (will still freeze your computer so... yeah)\n",
    "#if torch.backends.mps.is_available():\n",
    "#    print(\"MPS (Apple GPU) is available!\")\n",
    "#else:\n",
    "#    print(\"MPS is not available. Falling back to CPU.\")\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the datasets\n",
    "\n",
    "#this tokenizes each text\n",
    "tokenized_datasets = datasets.map(lambda x: tokenizer(x['text'], truncation=True), batched=True, remove_columns=['text'])\n",
    "\n",
    "# rename for multiclass fine-tuning\n",
    "tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "\n",
    "# set format to pytorch\n",
    "tokenized_datasets.set_format(type='torch')\n",
    "\n",
    "tokenized_datasets\n",
    "#labels are target class labels for classification, integers (0 to 3)\n",
    "#input ids are tokenized representations of the text input.\n",
    "#attention mask is binary mask (0s and 1s) indicating which tokens are actual words (1) and which are padding (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up data collator (this makes it so that your max sentance lengh is only as big as that batch, improving efficiency)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load datasets\n",
    "#Attempting to reduce batch size to 8\n",
    "\n",
    "dataloaders = {\n",
    "    'train': None,\n",
    "    'validation': None,\n",
    "    'test': None,\n",
    "}\n",
    "for dataset_type in ['train', 'validation', 'test']:\n",
    "    dataloaders[dataset_type] = DataLoader(\n",
    "        dataset = tokenized_datasets[dataset_type],\n",
    "        batch_size = 32,\n",
    "        shuffle = True,\n",
    "        collate_fn = data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define optimizer and scheduler and accuracy score\n",
    "\n",
    "#optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0, no_deprecation_warning=True)\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_training_steps = len(dataloaders['train']) * 3  # 3 epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "\n",
    "#model.parameters(): Optimizes all trainable parameters of BERT.\n",
    "#lr=5e-5: learning rate, standard for fine-tuning BERT.\n",
    "#weight_decay=0: Disables L2 regularization.\n",
    "#A learning rate scheduler adjusts the learning rate during training to improve performance.\n",
    "#AdamW improves gradient updates and prevents over-regularization.\n",
    "\n",
    "#accuracy_metric = evaluate.load('accuracy')\n",
    "#f1_metric = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training function\n",
    "#trains a BERT model for one epoch on a classification dataset using gradient descent\n",
    "#updates model parameters.\n",
    "\n",
    "def train(model, dataloader):\n",
    "    # set to train mode\n",
    "    model.train() #activates dropout and layer normalization\n",
    "    loss = 0\n",
    "    for batch in tqdm(dataloader): #Iterates over each mini-batch in the dataset and displays progress with tqdm.\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad() #Clears old gradients before backpropagation (otherwise, PyTorch accumulates gradients).\n",
    "        \n",
    "        # get predictions\n",
    "        batch = {k:v.to(device) for k,v in batch.items()} #Moves all tensors (input_ids, attention_mask, labels) to GPU or CPU.\n",
    "        outputs = model(**batch)  #Feeds input into BERT and gets outputs.\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)  #Converts logits to predicted class indices (highest probability). Logits (raw scores) for each class.\n",
    "        labels = batch['labels'] #these are the ground truth labels\n",
    "        \n",
    "        # gradient descent\n",
    "        outputs.loss.backward()  # Compute gradients\n",
    "        optimizer.step() # Update model parameters\n",
    "        lr_scheduler.step() # Updates the learning rate according to the scheduler.\n",
    "        \n",
    "        # accumulate metrics\n",
    "        loss += outputs.loss.item() #Adds batch loss to total loss.\n",
    "        accuracy_metric.add_batch(predictions=predictions, references=labels) #Stores batch predictions & labels for later accuracy calculation.\n",
    "        f1_metric.add_batch(predictions=predictions, references=labels) #Stores predictions for F1-score computation.\n",
    "    \n",
    "    # return metrics\n",
    "    loss /= len(dataloader) #Computes average loss over all batches.\n",
    "    accuracy = accuracy_metric.compute() # Calculates accuracy after processing all batches.\n",
    "    f1 = f1_metric.compute(average='macro')\n",
    "    return {'loss':loss, **accuracy, **f1} #Returns a dictionary with loss, accuracy, and F1-score.\n",
    "\n",
    "\n",
    "def trainfaster(model, dataloader):\n",
    "    model.train()  # Activate dropout and layer normalization\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(dataloader):  # Iterate over batches with progress bar\n",
    "        optimizer.zero_grad()  # Clear old gradients\n",
    "        \n",
    "        # Move batch to device (GPU or CPU)\n",
    "        #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        outputs = model(**batch)  # Forward pass\n",
    "        loss = outputs.loss  # Get loss\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update parameters\n",
    "        lr_scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Store loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Convert logits to class predictions\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "        # Store predictions and labels\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    # Compute final loss, accuracy, and F1-score\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "    return {'loss': avg_loss, 'accuracy': accuracy, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n",
    "# only measures performance without modifying the model.\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    # set to evaluation mode\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    # disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            # get predictions\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            labels = batch['labels'] \n",
    "\n",
    "            # accumulate metrics\n",
    "            loss += outputs.loss.item() \n",
    "            accuracy_metric.add_batch(predictions=predictions, references=labels)\n",
    "            f1_metric.add_batch(predictions=predictions, references=labels)\n",
    "    \n",
    "    # return metrics\n",
    "    loss /= len(dataloader)\n",
    "    accuracy = accuracy_metric.compute()\n",
    "    f1 = f1_metric.compute(average='macro')\n",
    "    return {'loss':loss, **accuracy, **f1}\n",
    "\n",
    "def evaluatefaster(model, dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss  # Get loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Convert logits to class predictions\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # Compute final loss, accuracy, and F1-score\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "    return {'loss': avg_loss, 'accuracy': accuracy, 'f1': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and fine-tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS CHAT GPTS TRAIN FUNCTION. I AM NOT USING IT\n",
    "\n",
    "def chat_train():\n",
    "    #epochs = 3\n",
    "    best_val_accuracy = 0\n",
    "\n",
    "    def train2(epochs):\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            loop = tqdm(dataloaders['train'], leave=True)\n",
    "            for batch in loop:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Compute training accuracy\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == batch[\"label\"]).sum().item()\n",
    "                total += batch[\"label\"].size(0)\n",
    "\n",
    "                loop.set_description(f\"Epoch {epoch+1}\")\n",
    "                loop.set_postfix(loss=total_loss / total, acc=correct / total)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_preds = []\n",
    "            val_labels = []\n",
    "            with torch.no_grad():\n",
    "                for batch in dataloaders['validation']:\n",
    "                    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                    outputs = model(**batch)\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                    val_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "                    val_labels.extend(batch[\"label\"].cpu().numpy())\n",
    "\n",
    "            val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "            print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_accuracy < best_val_accuracy:\n",
    "                print(\"Validation accuracy decreased, stopping training early.\")\n",
    "                break\n",
    "            best_val_accuracy = val_accuracy\n",
    "\n",
    "        print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the test data on the un-trained model to get baseline performance\n",
    "# Since the model hasn’t been trained, its weights are random, It will likely make random predictions.\n",
    "#The accuracy will be close to random chance: For a 4-class classification task, accuracy ≈ 25%.\n",
    "\n",
    "test_metrics = evaluatefaster(model, dataloaders['test'])\n",
    "print(f\"TEST ACCURACY: {test_metrics['accuracy']:.5f}\", end=\" ; \")\n",
    "print(f\"F1 (MACRO): {test_metrics['f1']:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test out the un-trained model on some examples\n",
    "\n",
    "model.eval()  # Ensure stable inference\n",
    "sentances =[\"Breaking news: AI is taking over!\", \"The HABS beat the senators 10-1 in a close game!\", \"Is Tesla coming out with a new car?\", \"I love cats\", \"Soccer World Cup Final Tomorrow!\"]\n",
    "\n",
    "for sentance in sentances:\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentance, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "        predicted_class = torch.argmax(outputs.logits, dim=-1)\n",
    "    print(f\"text: {sentance} \\npredicted label: {id2label[predicted_class.item()]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine-tune the model\n",
    "#THIS WAS RUN SEPERATELY ON MIMI, SO DON'T NEED TO RUN THIS AGAIN. PRE TRAINED MODEL IS SAVED IN SAVED_MODEL\n",
    "\n",
    "'''\n",
    "for epoch in range(3): #2 epochs\n",
    "    train_metrics = trainfaster(model, dataloaders['train'])\n",
    "    validation_metrics = evaluatefaster(model, dataloaders['validation'])\n",
    "          \n",
    "    print(f\"EPOCH {epoch+1}\", end=\" | \")\n",
    "    print(f\"TRAIN LOSS: {train_metrics['loss']:.5f}\", end=\" | \")\n",
    "    print(f\"VALIDATION LOSS: {validation_metrics['loss']:.5f}\", end=\" ; \")\n",
    "    print(f\"ACCURACY: {validation_metrics['accuracy']:.5f}\", end=\" ; \")\n",
    "    print(f\"F1 (MACRO): {validation_metrics['f1']:.5f}\")*/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the trained model\n",
    "#THIS WAS RUN SEPERATELY ON MIMI, SO DON'T NEED TO RUN THIS AGAIN. PRE TRAINED MODEL IS SAVED IN SAVED_MODEL\n",
    "\n",
    "'''\n",
    "model.cpu()\n",
    "model.save_pretrained(\"saved_model\")\n",
    "tokenizer.save_pretrained(\"saved_model\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the pre-trained model onto gpu\n",
    "\n",
    "trainedmodel = BertForSequenceClassification.from_pretrained(\"saved_model\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS (Apple GPU) is available!\")\n",
    "else:\n",
    "   print(\"MPS is not available. Falling back to CPU.\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "trainedmodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the fine-tuned model\n",
    "\n",
    "test_metrics = evaluatefaster(trainedmodel, dataloaders['test'])\n",
    "print(f\"TEST ACCURACY: {test_metrics['accuracy']:.5f}\", end=\" ; \")\n",
    "print(f\"F1 (MACRO): {test_metrics['f1']:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-define a classifier pipeline that packages all params into one model\n",
    "\n",
    "news_topic_classifier = pipeline(task='text-classification', model=trainedmodel, tokenizer=tokenizer, device=torch.device(\"mps\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
